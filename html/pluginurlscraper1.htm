<!DOCTYPE html PUBLIC "-//IETF//DTD HTML//EN">
<html>
  <head>
    <meta content="text/html; charset=UTF-8" http-equiv="content-type">
    <link rel="stylesheet" type="text/css" href="ph-helpstd.css">
	<link rel="stylesheet" type="text/css" href="ph-help.css">
    <title>URL Scraper1</title>
    <style type="text/css">
<!--
.style2 {color: #FF00FF}
-->
    </style>
</head>
  <body>
    <div class="ph1"> Plugin: URLScraper1 </div>
    <div class="indent2 padtop1"><STRONG>Launch Data (ActiveX 
Classname):</STRONG>     PH_URLScraper1.phurlscraper1</div>
    <div class="indent2 padtop1"><STRONG>Initialization Data:</STRONG>  Enter the full path and filename
      of the INI file for this particular instance of the File Monitor plugin.
      If you're using the defaults, then this value will be: c:\powerhome\plugins\phfilemon.ini</div>
    <div class="indent2 padtop1"> 
      <p>The PowerHome URL Scraper plugin is designed to parse data from designated web pages, in a manner similar to the ph_regex() family of string search functions, except that
      the plugin runs in the background in it's own independant thread, and takes no PowerHome resources.</p>
      <p>The regex search that is done uses the VBScript regular expression engine (the same that is used in the new ph_regex2 functions). Full details, and good tutorials, on the protocol can be found here ...<br><br>

&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://www.regular-expressions.info/quickstart.html" title="VB Script regex tutorial">http://www.regular-expressions.info/quickstart.html</a></p>
    </div>

    <div class="topic indent2 padtop1">Using the PowerHome URLScraper1 Plugin:</div>
    
	<p class="indent4">To use the scraper the Plugin must be ...<br>
  &nbsp;&nbsp;1) defined in PowerHome Explorer&gt;Setup&gt;Plugins, <br>
  &nbsp;&nbsp;2) the default initialization file (urlscraper.ini) &quot;c:\powerhome\ plugins&quot; modified or replaced, and <br>
  &nbsp;&nbsp;3)
	  a trigger event created in the PowerHome Explorer Triggers table (to call a Macro, or other Action). </p>
	<div class="topic indent3 padtop1">Defining the phfilemon.ini file:</div>
	<p class="indent4">A definition for the URLScraper1 plugin must be entered in the PowerHome Explorer&gt;Setup&gt;Plugins table as follows (the 'Load Order' entry doesn't have to be 10. Any sequence number, is appropriate) ...<br>
	  <br>
	  <img src="images/urlscrape1-plugins.png" alt="plugin definition" width="684" height="66">
    <div class="topic indent3 padtop1">Configuring the phfilemon.ini file:</div>
    <ol class="indent6
	">
      <li>Open the &quot;c:\powerhome\ plugins\urlscraper.ini&quot; file in Notepad. You'll need 
        to configure the URLScraper plugin actions by modifying that file. 
      <li>The scraper .ini file is comprised of two main parts (see example below), one of which may be repeating depending on the number and complexity of the &quot;scrapes.&quot;<br>
      <br>
      The 1st section is the [config] area, which has a single entry that defines how many different URL's will be defined to be searched.<br>
      <br>The 2nd section is comprised of a <em>primary</em> and one, or more, secondary <em>sub-sections</em>. The Primary part defines the URL address (eg, www.wund.com), how often (in <strong>minutes</strong>) it is to be scanned, and the number of 'scrapes' (there can be up to 9 defined in each sub-section) <br>
      <br>Each sub-section defines the regex search string, along with two parameters that define which (if multiple) matches is to be extracted, and a flag defining whether the search is case sensitive, or not.   
  </ol>
<div class="topic indent2 padtop1">
Example
</div>
<div class=ph11><br>
</div>
<div class=ph20>
  <p><strong>[config]</strong><br>
    urlcount=1<br>
  <div class="indent2 style2">The urlcount determines how many unique URL’s will be retrieved (NOTE: a single instance of the plugin can retrieve multiple different URL’s). For each URL in the URL count, you’ll have URL_x sections. That is, for URL count of 1, you’ll have a [URL_1] section. If you have a count of 2, you’ll have both a [URL_1} and [URL_2] section.</div>
    </p>
  <p><strong>[URL_1]</strong><br>
    url=http://m.wund.com/cgi-bin/findweather/getForecast?brand=mobile&amp;query=32712<br>
    freq=0.5<br>
    scrapecount=9<br>
  <div class="indent2 style2">Within a [URL_X] section, you’ll have the url, the freq (the frequency in <em><strong>minutes</strong></em> for how often to retrieve the URL), and the scrapecount. The scrapecount is how many regex searches are going to be made against the retrieved URL HTML data. For [URL_1] with a scrapecount of 2, you’ll have both a [URL_1_1] and [URL_1_2] section. If you have a [URL_2] section with a scrapecount of 1, then you’ll have only a [URL_2_1] section. </div>
  </p>
  <p><strong>[URL_1_1]</strong><br>
    ;Temperature <span class="style2">{The &quot;;&quot; makes this a comment line}</span> <br>
    regexsearch=&lt;td&gt;Temperature&lt;/td&gt;[\s\S]*?&lt;b&gt;(.+)&lt;/b&gt;<br>
  <div class="indent2 style2">The [URL_X_Y] section defines a regex search for the URL and fires a generic plugin trigger. The “X” value corresponds to the Trigger ID column (Command 1 for an X value of 1) and the “Y” value corresponds to the Trigger Value column (Option 1 for a Y value of 1).<br>
    <br>
  The search looks for the unique string of &quot; &lt;td&gt;Temperature&lt;/td&gt;&quot; and then captures everything &quot;[\s\S]&quot; that is either a 'whitespace character or not a whitespace character' (ie, everything) that occurs one or more times &quot;*&quot; and makes the search non-greedy &quot;?&quot; (that is, stop at the first match found, rather than looking for more).</div> 
    <br>
    regexoccur=1 <span class="style2">{Grab the first match found (actually redundant with the ? in the search string}</span> <br>
    regexflags=0 &nbsp;<span class="style2">{Use 0 for case insentive searchs and 1 for case sensetive searches.} </span><br>

    </p>
	
  <p><strong>[URL_1_2]</strong><br>
    ;Humidity<br>
    regexsearch=&lt;td&gt;Humidity&lt;/td&gt;[\s\S]*?&lt;b&gt;(.+)&lt;/b&gt;<br>
    regexoccur=1<br>
  regexflags=0</p>
  <p>These two searches will find the Temperature and the Humidity from the m.wund.com web site and return to the Trigger the Temperature in LOCAL1 and the Humidity in LOCAL2. Up to 9 parameters can be captured. If more are needed a second (ie, {URL_2] section could be defined to capture additional data.</p>
  <p>NOTE: The [URL_x] sections are independant of each other and thus can be used to capture additional data from the same site, as indicated above, or may be directed at totally different sites with different frequencies and scrape counts for different purposes (ie, weather on one and football scores on another). </p>
</div>
</div>
  <div class="topic indent3 padtop1">Configuring the Triggers:</div>
  <br>
  <p class="indent4">Create a new generic plugin trigger for each of the URL_x scrapes. You can give them ID’s of WEATHER1 and WEATHER2. Set the Trigger ID number to “Command 1" for both and the Trigger Value to “Option 1" for WEATHER1 and “Option 2" for WEATHER2. <br><br>

When the WEATHER1 trigger fires, the Temperature will be in LOCAL1, and the Humidity in LOCAL2. Up to 9 search matches can be captured in a single trigger. If more are needed, then a 2nd URL_x (ie, URL_S) must be defined. It can pass nine more data elements. <br>
<br>
<img src="images/urlscrape1-trig.png" alt="URL Scrape Trigger" width="952" height="65"><br>
<br>
  
  </body>
</html>
